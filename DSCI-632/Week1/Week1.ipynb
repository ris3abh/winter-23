{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "monthly-beginning",
   "metadata": {},
   "source": [
    "## Technical Setup for using Pyspark on Cloud or Local Machine\n",
    "\n",
    "- This course is about using cloud technologies (such as Google Cloud Platform (GCP), Amazon Web Services (AWS) and Databricks) for Big Data Processing and Analytics (using mainly Spark).\n",
    "\n",
    "- In this course, we use Spark (its Python API called Pyspark) for Big Data\n",
    "\n",
    "- There are many options to setup Pyspark on Cloud services such as Google Cloud Computing (GCP) or Google Colabratory (Google Colab), DataBricks, Amazon Web Services (AWS) or Local Machine \n",
    "\n",
    "<img src=\"Big_Data_Components.png\" width=\"450\" height=\"450\">\n",
    "\n",
    "Here, when there exist computational resources (like CPUs, RAMs, etc), big data platforms such as Spark or Hadoop are capabale to distribute the analytical computations among those hardwares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-transition",
   "metadata": {},
   "source": [
    "## Setup Pyspark in Google Colab\n",
    "\n",
    "- Do the following steps, you can write down Pyspark code on Google Colab\n",
    "- Disclaimer: Google Colab does not provide resources for Big Data Processing \n",
    "- A drawback of this method: Everytime, when we close the Notebook and return back to the code later, we should do the steps again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "- !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# - !wget https://mirror.jframeworks.com/apache/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n",
    "- !wget https://dlcdn.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz    \n",
    "    https://spark.apache.org/downloads.html -> click on Download Spark: spark-3.0.3-bin-hadoop2.7.tgz\n",
    "- !tar xvf spark-3.0.3-bin-hadoop2.7.tgz\n",
    "- !pip install -q findspark\n",
    "- !pip install pyspark==3.0.3\n",
    "- import os\n",
    "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "  os.environ[\"SPARK_HOME\"] = \"spark-3.0.3-bin-hadoop2.7\"\n",
    "- from pyspark.sql import SparkSession\n",
    "    spark= SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate() \n",
    "    data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "    rdd_sample = spark.sparkContext.parallelize(data, 2)\n",
    "    rdd_sample.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-playback",
   "metadata": {},
   "source": [
    "## Use Pyspark in Dataproc (GCP)\n",
    "\n",
    "- Dataproc is a managed service to run Hadoop and Spark jobs\n",
    "- A Dataproc cluster comes with preinstalled Spark\n",
    "- We will typically use the PySpark REPL or spark-submit with Python scripts\n",
    "- Caution – delete you cluster when you are done working or you will waste all of your Google credits!\n",
    "- Step 1 – create a Google account\n",
    "- Step 2 – set up the Google cloud account\n",
    "\n",
    "    Redeem your credits\n",
    "    \n",
    "- visit console.cloud.google.com\n",
    "- Follow the steps in `Intro_to_Google_Cloud.pptx` and `Technical_Setup.pptx`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-request",
   "metadata": {},
   "source": [
    "### Dataproc Online Tutorials:\n",
    "    \n",
    "- How to Create Google Cloud Dataproc Clusters for Spark: https://www.youtube.com/watch?v=nccCsk_MHDs\n",
    "- Apache Spark & Jupyter on Google Cloud Dataproc Cluster: https://www.youtube.com/watch?v=5OYT2SSMGo8\n",
    "- Using PySpark on Dataproc Hadoop Cluster to process large CSV file: https://www.youtube.com/watch?v=Y6OXGc0mmYM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-system",
   "metadata": {},
   "source": [
    "## If we want to config or read data from GPC from our local machine\n",
    "\n",
    "- Install the gcloud CLI: https://cloud.google.com/sdk/docs/install: This is optional. Instead of doing the steps on GCP manually, we can configure anything by command line from our local machine (laptop) \n",
    "\n",
    "- For example, dumpig a csv file to GCP Storage and do data analysis locally\n",
    "\n",
    "<img src=\"Storage_GCP.png\" width=\"800\" height=\"800\">\n",
    "\n",
    "`pip install gcsfs`\n",
    "\n",
    "`df = pd.read_csv('gs://angular-amp-303119/Data/Churn_Modelling.csv') -> Make The Data Public`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-twenty",
   "metadata": {},
   "source": [
    "## Use Databricks for Pyspark\n",
    "\n",
    "- Databricks provides you with a free trial to their notebooks\n",
    "\n",
    "- Create account here for Databricks: https://community.cloud.databricks.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-blackberry",
   "metadata": {},
   "source": [
    "## Config local machine for Pyspark\n",
    "\n",
    "- By doing the following steps, you can have Pyspark on linux local machine\n",
    "- The purpose of doing this is learning Pyspark syntaxes only\n",
    "- Because in most of the cases, we do not have resources in our local machine, the pyspark process will not be fast, so here learning the Pyspark syntaxes is the objective task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "Spark on local machine:\n",
    "\n",
    "1- Install jdk 8 or higher via https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\n",
    "2- tar zxvf jdk-8u281-linux-x64.tar.gz\n",
    "3- pip install pyspark\n",
    "4- pip install findspark\n",
    "5- optional: sudo gedit ~/.bashrc\n",
    "6- Download Hadoop from https://spark.apache.org/downloads.html, then tar zxvf spark-3.0.2-bin-hadoop2.7.tgz\n",
    "7- python (type it in terminal)\n",
    "8- import findspark\n",
    "9- findspark.init('~/Downloads/spark-3.0.2-bin-hadoop2.7/bin')\n",
    "10-exit()\n",
    "11-pyspark (type it in the terminal)\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "ls = [1,2,3,4]\n",
    "rdd_sample = sc.parallelize(ls, 2)\n",
    "rdd_sample.take(2)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-grade",
   "metadata": {},
   "source": [
    "## AWS\n",
    "\n",
    "- We can have Jupyter Notebook on AWS from its SageMaker Service with pre-installed Pyspark\n",
    "\n",
    "- We can dump large dataset on S3 (Simple Storage Service) and read it by using Boto -> Do the followings in the terminal on your local machine:\n",
    "\n",
    "    - pip install awscli \n",
    "\n",
    "    - $ aws configure \n",
    "\n",
    "    - AWS Access Key ID [None]: ...\n",
    "\n",
    "    - AWS Secret Access Key [None]: ...\n",
    "\n",
    "    - Default region name [None]: ...\n",
    "\n",
    "    - Default output format [None]: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access to our dataset on S3 and create Pandas data frame from it\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "bucket = \"makeschooldata\"\n",
    "file_name = \"data/Churn_Modelling.csv\"\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "# 's3' is a key word. create connection to S3 using default config and all buckets within S3\n",
    "\n",
    "obj = s3.get_object(Bucket=bucket, Key=file_name)\n",
    "# get object and file (key) from bucket\n",
    "\n",
    "df = pd.read_csv(obj['Body']) # 'Body' is a keyword\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
