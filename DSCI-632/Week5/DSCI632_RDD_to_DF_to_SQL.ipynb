{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity:\n",
    "\n",
    "- Create a Pyspark DataFrame from people.txt file which has two columns name and age\n",
    "- Hint: first create RDD and then create its Pyspark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/pyspark/sql/context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ[\"JAVA_HOME\"]='/usr/local/opt/openjdk@8'\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate() \n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark import SparkContext\n",
    "# from pyspark.sql import SQLContext, Row\n",
    "\n",
    "# sc = SparkContext()\n",
    "# sqlContext = SQLContext(sc)\n",
    "# spark.SparkContext.textFile(\"people.txt\")\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael, 29', 'Andy, 30', 'Justin, 19']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Michael', ' 29'], ['Andy', ' 30'], ['Justin', ' 19']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Michael', age=29),\n",
       " Row(name='Andy', age=30),\n",
       " Row(name='Justin', age=19)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sqlContext = SQLContext(sc)\n",
    "df_people = sqlContext.createDataFrame(people)\n",
    "df_people.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Often we might want to store the spark Data frame as the table (SQL table) and query it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people.createOrReplaceTempView(\"DB_people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(name='Justin')\n"
     ]
    }
   ],
   "source": [
    "teenagers = sqlContext.sql(\"SELECT name FROM DB_people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
    "# teenNames = teenagers.map(lambda p: \"Name: \" + p.name)\n",
    "for teenName in teenagers.collect():\n",
    "    print(teenName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teenagers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(teenagers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable       Type            Data/Info\n",
      "----------------------------------------\n",
      "Row            type            <class 'pyspark.sql.types.Row'>\n",
      "SQLContext     type            <class 'pyspark.sql.context.SQLContext'>\n",
      "SparkContext   type            <class 'pyspark.context.SparkContext'>\n",
      "df_people      DataFrame       DataFrame[name: string, age: bigint]\n",
      "lines          RDD             people.txt MapPartitionsR<...>MethodAccessorImpl.java:0\n",
      "parts          PipelinedRDD    PythonRDD[2] at collect a<...>n-input-3-c82236e59854>:1\n",
      "people         PipelinedRDD    PythonRDD[3] at RDD at PythonRDD.scala:53\n",
      "sc             SparkContext    <SparkContext master=loca<...>*] appName=pyspark-shell>\n",
      "sqlContext     SQLContext      <pyspark.sql.context.SQLC<...>object at 0x7f99c1d62670>\n",
      "teenName       Row             Row(name='Justin')\n",
      "teenagers      DataFrame       DataFrame[name: string]\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.cacheTable('DB_people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable       Type            Data/Info\n",
      "----------------------------------------\n",
      "Row            type            <class 'pyspark.sql.types.Row'>\n",
      "SQLContext     type            <class 'pyspark.sql.context.SQLContext'>\n",
      "SparkContext   type            <class 'pyspark.context.SparkContext'>\n",
      "df_people      DataFrame       DataFrame[name: string, age: bigint]\n",
      "lines          RDD             people.txt MapPartitionsR<...>MethodAccessorImpl.java:0\n",
      "parts          PipelinedRDD    PythonRDD[2] at collect a<...>n-input-3-c82236e59854>:1\n",
      "people         PipelinedRDD    PythonRDD[3] at RDD at PythonRDD.scala:53\n",
      "pyspark        module          <module 'pyspark' from '/<...>ges/pyspark/__init__.py'>\n",
      "sc             SparkContext    <SparkContext master=loca<...>*] appName=pyspark-shell>\n",
      "sqlContext     SQLContext      <pyspark.sql.context.SQLC<...>object at 0x7f99c1d62670>\n",
      "teenName       Row             Row(name='Justin')\n",
      "teenagers      DataFrame       DataFrame[name: string]\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/07 16:40:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the people.txt file as RDD and convert it to DataFrame\n",
    "\n",
    "lines = sc.textFile(\"people.txt\")\n",
    "\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "\n",
    "df_people = sqlContext.createDataFrame(people)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (main, Dec 23 2022, 09:28:24) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
