{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52873,"status":"ok","timestamp":1610801734298,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"JpVv_wqkQleX","outputId":"38b06896-b84c-4d9d-80ae-2e38e2e95348"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","nbdir = \"/content/gdrive/My Drive/DSCI521/Colab/02-textual/\""]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52869,"status":"ok","timestamp":1610801734301,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"FBOjRlBgQqjY","outputId":"4adb847f-c83f-455a-858e-50d1145eac3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/gdrive/My Drive/DSCI521/Colab/02-textual\n"]}],"source":["%cd /content/gdrive/My\\ Drive/DSCI521/Colab/02-textual/"]},{"cell_type":"markdown","metadata":{"id":"iN0D_7g6t64v"},"source":["# DSCI 521: Methods for analysis and interpretation <br>Chapter 2: Feature engineering and language processing\n","\n","## Exercises\n","Note: numberings refer to the main notes."]},{"cell_type":"markdown","metadata":{"id":"7FfYvUBut641"},"source":["#### 2.1.1.3 Exercise: Regex phone numbers\n","Read the file `phone-numbers.txt`. It contains a phone number in each line. \\[Hint: use something like `lines = open(\"file.txt\", \"r\").readlines()`\\] Store only the phone numbers with the area code \"215\" in a list and print it out. Use regex-based pattern matching, not any other methods which occur to you."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53396,"status":"ok","timestamp":1610801734839,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"maGokUk1t642","outputId":"fb59b43d-feab-44bd-8b3d-6e7b61ff8567"},"outputs":[{"data":{"text/plain":["['215-345-3463', '215-756-8273']"]},"execution_count":3,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["import re\n","document = open(\"./data/phone-numbers.txt\", \"r\").read()\n","\n","numbers = re.findall('215-[0-9][0-9][0-9]-[0-9][0-9][0-9][0-9]', \n","                     document)\n","numbers"]},{"cell_type":"markdown","metadata":{"id":"-bY1gFqlt644"},"source":["#### 2.1.1.8 Exercise: Names of the gods\n","In the cell below is some text. It's an extract from [A Clash of Kings](https://www.goodreads.com/book/show/10572.A_Clash_of_Kings), specifically, about a character's prayer to some fictional gods. Use regex to extract the names of these gods. Your output should be a list that looks something like `[\"the Father\", \"the Mother\", \"the Warrior\"]`."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53389,"status":"ok","timestamp":1610801734841,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"AJ251UJ9t645","outputId":"8fa9238f-a224-491c-b6bf-071210b9989f"},"outputs":[{"data":{"text/plain":["['the Smith', 'the Maid', 'the Father', 'the Warrior', 'the Crone']"]},"execution_count":4,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["text = 'Lost and weary, Catelyn Stark gave herself over to her gods. She knelt before the Smith, who fixed things that were broken, and asked that he give her sweet Bran his protection. She went to the Maid and beseeched her to lend her courage to Arya and Sansa, to guard them in their innocence. To the Father, she prayed for justice, the strength to seek it and the wisdom to know it, and she asked the Warrior to keep Robb strong and shield him in his battles. Lastly she turned to the Crone, whose statues often showed her with a lamp in one hand. \"Guide me, wise lady,\" she prayed. \"Show me the path I must walk, and do not let me stumble in the dark places that lie ahead.\"'\n","\n","gods = re.findall(\"the [A-Z][a-z]+\", text)\n","\n","gods"]},{"cell_type":"markdown","metadata":{"id":"fdWSdF77t645"},"source":["#### 2.1.2.4 Exercise: Improving a regex-based sentence tokenizer\n","First, write a few sentences in a complex (but grammatically acceptable) way so that the (above) regex-based tokenizer breaks. Then, fix the pattern so that the tokenizer can handle your text appropriately."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53381,"status":"ok","timestamp":1610801734841,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"I2Kvi9Brt646","outputId":"314fad54-e062-4abc-d3fb-5aa1ff83f305"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n","  return _compile(pattern, flags).split(string, maxsplit)\n"]},{"data":{"text/plain":["[\"With all due resp., I don't think this is a very good tokenization!\",\n"," \"Here's another one!\"]"]},"execution_count":5,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["## regex-based sentence tokenizer\n","sentences = \"With all due resp., I don't think this is a very good tokenization! Here's another one!\"\n","sentences_tokenized = re.split(\"\\s*(?<=[\\.\\?\\!][^a-zA-Z0-9,])\\s*\", sentences)\n","sentences_tokenized"]},{"cell_type":"markdown","metadata":{"id":"GkH2EXn1t646"},"source":["#### 2.1.3.2 Exercise: POS tagging \n","Apply POS tagging to a sentence of your choosing and filter for only verbs and nouns."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55422,"status":"ok","timestamp":1610801736891,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"-WUKanr9t647","outputId":"75de2161-de1f-4497-e0f2-33d067ebd2a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["token\tcoarse\tfine\n","Use\tVERB\tVB\n","test\tNOUN\tNN\n","sentences\tNOUN\tNNS\n","Joey\tPROPN\tNNP\n"]}],"source":["import spacy\n","import en_core_web_sm\n","nlp = en_core_web_sm.load()\n","\n","running_sentence = \"Use some of our test sentences; Joey's not very smart, nor charming.\"\n","doc = nlp(running_sentence)\n","\n","print(\"token\\tcoarse\\tfine\")\n","for token in doc:\n","    if token.pos_ in {\"NOUN\", \"VERB\", \"PROPN\"}:\n","        print(token.text + \"\\t\" + token.pos_ + \"\\t\" + token.tag_)"]},{"cell_type":"markdown","metadata":{"id":"Wv3N3rjct647"},"source":["#### 2.1.3.5 Exercise: using grammar for information extraction\n","Apply the spacy grammatical parsing and extract any subject-verb token pairs."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55416,"status":"ok","timestamp":1610801736894,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"jjaLTvIKt648","outputId":"e7d5937b-0d8b-4abe-ccb7-ef42650fb166"},"outputs":[{"name":"stdout","output_type":"stream","text":["subject\tverb\n","'s use\n","we meet\n"]}],"source":["running_sentence = \"Let's use another one. Anything else? Happy hour is tomorrow at 5:30 at Tap House where we will all meet up and say hi.\"\n","doc = nlp(running_sentence)\n","\n","print(\"subject\\tverb\")\n","for token in doc:\n","    if token.dep_ == \"nsubj\" and token.head.pos_ == \"VERB\":\n","        print(token.text + \" \"+ token.head.text)"]},{"cell_type":"markdown","metadata":{"id":"ur3A5wbRt649"},"source":["#### 2.1.4.4 Exercise: improved word frequency representation\n","Build a stop word list and lemmatization strategy (potentially using POS tags) to compute 'better' word frequencies, as you see fit."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55410,"status":"ok","timestamp":1610801736897,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"C_qH6Yx5t64-","outputId":"adbadecb-5544-48d3-86f5-7c5317bdef5a"},"outputs":[{"data":{"text/plain":["[('model', 4),\n"," ('word', 3),\n"," ('be', 3),\n"," ('BOW', 3),\n"," ('text', 2),\n"," ('as', 2),\n"," ('simply', 2),\n"," ('use', 2),\n"," ('frequency', 1),\n"," ('probably', 1),\n"," ('first', 1),\n"," ('easy', 1),\n"," ('numerical', 1),\n"," ('representation', 1),\n"," ('compute', 1),\n"," ('community', 1),\n"," ('refer', 1),\n"," ('bag', 1),\n"," ('Put', 1),\n"," ('count', 1),\n"," ('number', 1),\n"," ('time', 1),\n"," ('appear', 1),\n"," ('document', 1),\n"," ('course', 1)]"]},"execution_count":8,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["from collections import Counter\n","\n","text = \"\"\"Word frequencies are probably the first and easiest \n","numerical representation of text to compute. In some communities, \n","this is referred to as the bag of words (BOW) model. \n","Put simply, the BOW model simply counts up the \n","number of times each word appears in a document. \n","This of course depends on a few things, e.g., case and lemmatization. \n","However, constructing a basic BOW model is quite straightforward, especially using `Counter`. \n","Let's use this very paragraph as our example text for the BOW model.\"\"\"\n","\n","# in addition to excluding stop words, let's also exclude specific parts of speech, like determiners, particles,\n","# punctuation, and adpositions.\n","\n","stop_words = {'\\n', ',', '.', '`', 'the', 'and', 'of'}\n","excluded_pos = {\"DET\", \"PART\", \"PUNCT\", \"ADP\"}\n","\n","doc = nlp(text)\n","word_counts = Counter()\n","\n","for word in doc:\n","    if word.lemma_ not in stop_words and word.pos_ not in excluded_pos:\n","        word_counts[(word.lemma_)] += 1\n","\n","word_counts.most_common(25)"]},{"cell_type":"markdown","metadata":{"id":"bX49XzwWt64-"},"source":["#### 2.1.6.5 Exercise: exploring TF-IDF\n","Rank each of the example TF-IDF matrix's rows by TF-IDF values from high-to-low and interpret the kinds of words that have high TF-IDF values, i.e., are 'more important'. What about the low values, what kinds of words are these?"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":55409,"status":"ok","timestamp":1610801736899,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"oZjm2OHyt64-"},"outputs":[],"source":["import numpy as np\n","\n","def count_words(sentence):\n","    frequency = Counter()\n","    for word in sentence:\n","        frequency[word.text.lower()] += 1\n","    return frequency\n","\n","text = '''Lost and weary, Catelyn Stark gave herself over to her gods. \n","She knelt before the Smith, who fixed things that were broken, \n","and asked that he give her sweet Bran his protection. \n","She went to the Maid and beseeched her to lend her courage to Arya and Sansa, \n","to guard them in their innocence. \n","To the Father, she prayed for justice, the strength to seek it and the wisdom to know it, \n","and she asked the Warrior to keep Robb strong and shield him in his battles. \n","Lastly she turned to the Crone, whose statues often showed her with a lamp in one hand. \n","\"Guide me, wise lady,\" she prayed. \n","\"Show me the path I must walk, and do not let me stumble in the dark places that lie ahead.\"\n","'''\n","\n","doc = nlp(text)\n","    \n","## the 'master' set, keeps track of the words in all documents\n","all_words = set()\n","\n","## store the word frequencies by book\n","all_doc_frequencies = {}\n","\n","## loop over the sentences\n","for j, sentence in enumerate(doc.sents):\n","    frequency = count_words(sentence)\n","    all_doc_frequencies[j] = frequency\n","    doc_words = set(frequency.keys())\n","    all_words = all_words.union(doc_words)\n","    \n","## create a matrix of zeros: (words) x (documents)\n","TDM = np.zeros((len(all_words),len(all_doc_frequencies)))\n","## fix a word ordering for the rows\n","all_words = sorted(list(all_words))\n","## loop over the (sorted) document numbers and (ordered) words; fill in matrix\n","for j in all_doc_frequencies:\n","    for i, word in enumerate(all_words):\n","        TDM[i,j] = all_doc_frequencies[j][word]\n","\n","num_docs = TDM.shape[1]\n","\n","## start off with a copy of our TDM (frequencies)\n","TFIDF = np.array(TDM)\n","## loop over words\n","for i, word in enumerate(all_words):\n","    ## count docs containing the word\n","    num_docs_containing_word = len([x for x in TDM[i] if x])\n","    ### computen the inverse document frequence of this word\n","    IDF = -np.log2(num_docs_containing_word/num_docs)\n","    ## multiply this row by the IDF to transform it to TFIDF\n","    TFIDF[i,] = TFIDF[i,]*IDF"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55404,"status":"ok","timestamp":1610801736901,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"J79i7fmEt64-","outputId":"fb0c836b-ccc6-47fb-acb7-d39196268130"},"outputs":[{"name":"stdout","output_type":"stream","text":["For document #0, words ranked according to TF-IDF are:\n","\n","catelyn\t2.8074\n","gave\t2.8074\n","gods\t2.8074\n","herself\t2.8074\n","lost\t2.8074\n","over\t2.8074\n","stark\t2.8074\n","weary\t2.8074\n","her\t0.8074\n","to\t0.8074\n","and\t0.4854\n","\n","For document #1, words ranked according to TF-IDF are:\n","\n","that\t3.6147\n","before\t2.8074\n","bran\t2.8074\n","broken\t2.8074\n","fixed\t2.8074\n","give\t2.8074\n","he\t2.8074\n","knelt\t2.8074\n","protection\t2.8074\n","smith\t2.8074\n","sweet\t2.8074\n","things\t2.8074\n","were\t2.8074\n","who\t2.8074\n","asked\t1.8074\n","his\t1.8074\n","her\t0.8074\n","and\t0.4854\n","she\t0.4854\n","the\t0.4854\n","\n","For document #2, words ranked according to TF-IDF are:\n","\n","to\t3.2294\n","arya\t2.8074\n","beseeched\t2.8074\n","courage\t2.8074\n","guard\t2.8074\n","innocence\t2.8074\n","lend\t2.8074\n","maid\t2.8074\n","sansa\t2.8074\n","their\t2.8074\n","them\t2.8074\n","went\t2.8074\n","her\t1.6147\n","and\t0.9709\n","in\t0.8074\n","she\t0.4854\n","the\t0.4854\n","\n","For document #3, words ranked according to TF-IDF are:\n","\n","it\t5.6147\n","to\t3.2294\n","battles\t2.8074\n","father\t2.8074\n","for\t2.8074\n","him\t2.8074\n","justice\t2.8074\n","keep\t2.8074\n","know\t2.8074\n","robb\t2.8074\n","seek\t2.8074\n","shield\t2.8074\n","strength\t2.8074\n","strong\t2.8074\n","warrior\t2.8074\n","wisdom\t2.8074\n","the\t1.9417\n","asked\t1.8074\n","his\t1.8074\n","prayed\t1.8074\n","and\t1.4563\n","she\t0.9709\n","in\t0.8074\n","\n","For document #4, words ranked according to TF-IDF are:\n","\n","a\t2.8074\n","crone\t2.8074\n","hand\t2.8074\n","lamp\t2.8074\n","lastly\t2.8074\n","often\t2.8074\n","one\t2.8074\n","showed\t2.8074\n","statues\t2.8074\n","turned\t2.8074\n","whose\t2.8074\n","with\t2.8074\n","her\t0.8074\n","in\t0.8074\n","to\t0.8074\n","she\t0.4854\n","the\t0.4854\n","\n","For document #5, words ranked according to TF-IDF are:\n","\n","\"\t3.6147\n","guide\t2.8074\n","lady\t2.8074\n","wise\t2.8074\n","me\t1.8074\n","prayed\t1.8074\n","she\t0.4854\n","\n","For document #6, words ranked according to TF-IDF are:\n","\n","\"\t3.6147\n","me\t3.6147\n","ahead\t2.8074\n","dark\t2.8074\n","do\t2.8074\n","i\t2.8074\n","let\t2.8074\n","lie\t2.8074\n","must\t2.8074\n","not\t2.8074\n","path\t2.8074\n","places\t2.8074\n","show\t2.8074\n","stumble\t2.8074\n","walk\t2.8074\n","that\t1.8074\n","the\t0.9709\n","in\t0.8074\n","and\t0.4854\n","\n"]}],"source":["for j in range(TFIDF.shape[1]):\n","    doc_vals = TFIDF[:,j]\n","    \n","    # make word and TF-IDF value tuples, put them in a list, sort the list according to TF-IDF values, then only keep words with non-zero TF-IDF \n","    \n","    words_and_vals = [(word, val) for word, val in sorted(zip(all_words, doc_vals), key = lambda x: x[1], reverse = True) if val]\n","    print(\"For document #\" + str(j) + \", words ranked according to TF-IDF are:\\n\")\n","    for word, val in words_and_vals:\n","        print(word + \"\\t\" + str(round(val, 4)))\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"uOVgeU7nt64-"},"source":["It seems that words that are rare across documents have higher TF-IDF values. The lower the TF-IDF value, the more common the word."]},{"cell_type":"markdown","metadata":{"id":"_-1p47ipglpg"},"source":["## Additional In-depth Exercises\n","\n","### A. Constructing co-occurrence matrix statistics\n","\n","#### A.1 Build a tokenizer\n","To start, build a tokenization function called `tokens = tokenize(text, space = False)` that accepts a string called `text`, in addition to a boolean argument called `space`, which if positive will allow the tokenize function to determine if whitespace characters (at all) should be stored as a part of the list of `tokens` output.\n","\n","For this part of the exercise, use the character-class `'[0-9a-zA-Z'-]'` (or it's complimentary character class) to split on non-delimiters, but be sure to capture all portions of the text that are 'split' using a grouping mechanism. Likewise, ensure that all non-word-type tokens are completely resolved, e.g., there _shouldn't_ be any tokens which consist of multiple punctuation characters, such as `\".\\\"\"`, which should be sub-divided into multiple tokens.\n","\n","Likewise, be sure to collapse any multiple whitespace `\" \"` characters down to just one as an initial pre-processing step to the `text`."]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":55401,"status":"ok","timestamp":1610801736903,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"uf3vQm78y8fG"},"outputs":[],"source":["def tokenize(text, space = False):\n","    text = re.sub(\"[ ]+\", \" \", text)\n","    tokens = []\n","    for token in re.split(\"([0-9a-zA-Z'-]+)\", text):\n","        if not space:\n","            token = re.sub(\"[ ]+\", \"\", token)\n","        if not token:\n","            continue\n","        if re.search(\"[0-9a-zA-Z'-]\", token):                    \n","            tokens.append(token)\n","        else: \n","            tokens.extend(token)\n","    return tokens"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55395,"status":"ok","timestamp":1610801736905,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"TbPBnrQDQiC_","outputId":"83a7ca4b-8fac-4c2e-ecb9-1ccd792a3e7c"},"outputs":[{"data":{"text/plain":["['This',\n"," ' ',\n"," 'is',\n"," ' ',\n"," 'an',\n"," ' ',\n"," 'example',\n"," ' ',\n"," 'with',\n"," ' ',\n"," 'the',\n"," ' ',\n"," 'space',\n"," ' ',\n"," 'flag',\n"," ' ',\n"," 'as',\n"," ' ',\n"," 'True',\n"," '!']"]},"execution_count":12,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["tokenize('This is an example with the space flag as True!', True)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55389,"status":"ok","timestamp":1610801736906,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"CRFlA5n0QiDA","outputId":"f2f1fee2-beb0-4714-cbf4-6c6bdb648bf5"},"outputs":[{"data":{"text/plain":["['This',\n"," 'is',\n"," 'an',\n"," 'example',\n"," 'with',\n"," 'the',\n"," 'space',\n"," 'flag',\n"," 'as',\n"," 'False',\n"," '!']"]},"execution_count":13,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["tokenize('This is an example with the space flag as False!', False)"]},{"cell_type":"markdown","metadata":{"id":"YSQ7l-D6QiDB"},"source":["#### A.2 Build a word-sentence tokenizer\n","Here, the goal will be to produce a two-level tokenization utility that is similar to what Spacy produces:"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55380,"status":"ok","timestamp":1610801736907,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"rZTPpPDqYLtV","outputId":"725d3d56-af06-4a90-b608-27ca6e5db5d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Lost', 'and', 'weary', ',', 'Catelyn', 'Stark', 'gave', 'herself', 'over', 'to', 'her', 'gods', '.', '\\n']\n","['She', 'knelt', 'before', 'the', 'Smith', ',', 'who', 'fixed', 'things', 'that', 'were', 'broken', ',', '\\n', 'and', 'asked', 'that', 'he', 'give', 'her', 'sweet', 'Bran', 'his', 'protection', '.', '\\n']\n","['She', 'went', 'to', 'the', 'Maid', 'and', 'beseeched', 'her', 'to', 'lend', 'her', 'courage', 'to', 'Arya', 'and', 'Sansa', ',', '\\n', 'to', 'guard', 'them', 'in', 'their', 'innocence', '.', '\\n']\n","['To', 'the', 'Father', ',', 'she', 'prayed', 'for', 'justice', ',', 'the', 'strength', 'to', 'seek', 'it', 'and', 'the', 'wisdom', 'to', 'know', 'it', ',', '\\n', 'and', 'she', 'asked', 'the', 'Warrior', 'to', 'keep', 'Robb', 'strong', 'and', 'shield', 'him', 'in', 'his', 'battles', '.', '\\n']\n","['Lastly', 'she', 'turned', 'to', 'the', 'Crone', ',', 'whose', 'statues', 'often', 'showed', 'her', 'with', 'a', 'lamp', 'in', 'one', 'hand', '.', '\\n']\n","['\"', 'Guide', 'me', ',', 'wise', 'lady', ',', '\"', 'she', 'prayed', '.', '\\n']\n","['\"', 'Show', 'me', 'the', 'path', 'I', 'must', 'walk', ',', 'and', 'do', 'not', 'let', 'me', 'stumble', 'in', 'the', 'dark', 'places', 'that', 'lie', 'ahead', '.', '\"', '\\n']\n"]}],"source":["for s in doc.sents:\n","    print([w.text for w in s])"]},{"cell_type":"markdown","metadata":{"id":"jaDdM_w7QiDC"},"source":["with the caveat that we use our own tokenization utility (which can be flagged to retain space characters).\n","Since this will then require the utilization of a sentence tokenizer, download `nltk` (if you haven't already) and utilize its `sent_tokenize()` function."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1143,"status":"ok","timestamp":1610801778894,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"kIAULttrGrBt","outputId":"10369478-d154-4c45-bb7f-7188b8bf52cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["from nltk import sent_tokenize\n","import nltk\n","nltk.download('punkt')\n","\n","def word_sentence_tokenize(text, space = False):\n","    sentences = []\n","    for sentence in sent_tokenize(text):\n","        sentences.append(tokenize(sentence.strip(), space))\n","    return sentences"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":647,"status":"ok","timestamp":1610801786916,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"nC_67lQLQiDD","outputId":"08752183-78b0-4632-9e58-9d1c6e76494d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[['Lost', 'and', 'weary', ',', 'Catelyn', 'Stark', 'gave', 'herself', 'over', 'to', 'her', 'gods', '.'], ['She', 'knelt', 'before', 'the', 'Smith', ',', 'who', 'fixed', 'things', 'that', 'were', 'broken', ',', '\\n', 'and', 'asked', 'that', 'he', 'give', 'her', 'sweet', 'Bran', 'his', 'protection', '.'], ['She', 'went', 'to', 'the', 'Maid', 'and', 'beseeched', 'her', 'to', 'lend', 'her', 'courage', 'to', 'Arya', 'and', 'Sansa', ',', '\\n', 'to', 'guard', 'them', 'in', 'their', 'innocence', '.'], ['To', 'the', 'Father', ',', 'she', 'prayed', 'for', 'justice', ',', 'the', 'strength', 'to', 'seek', 'it', 'and', 'the', 'wisdom', 'to', 'know', 'it', ',', '\\n', 'and', 'she', 'asked', 'the', 'Warrior', 'to', 'keep', 'Robb', 'strong', 'and', 'shield', 'him', 'in', 'his', 'battles', '.'], ['Lastly', 'she', 'turned', 'to', 'the', 'Crone', ',', 'whose', 'statues', 'often', 'showed', 'her', 'with', 'a', 'lamp', 'in', 'one', 'hand', '.'], ['\"', 'Guide', 'me', ',', 'wise', 'lady', ',', '\"', 'she', 'prayed', '.'], ['\"', 'Show', 'me', 'the', 'path', 'I', 'must', 'walk', ',', 'and', 'do', 'not', 'let', 'me', 'stumble', 'in', 'the', 'dark', 'places', 'that', 'lie', 'ahead', '.', '\"']]\n"]}],"source":["print(word_sentence_tokenize(text))"]},{"cell_type":"markdown","metadata":{"id":"_Zzh8vtXQiDD"},"source":["#### A.3 Try to re-construct the document\n","Now that we have the two-stage tokenizer which can retain space characters, let's try an re-construct a document from its tokenization, with and without `space=True`.\n","\n","In particular, consider how to re-join the elements of the two-level list (sentences) of lists (words) of strings by a delimiter so as to re-construct the document."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"elapsed":402,"status":"ok","timestamp":1610801790166,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"8f_LqHV8QiDE","outputId":"d4c947c5-2c83-4df4-d350-a518a6c3b491"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Lost and weary, Catelyn Stark gave herself over to her gods.She knelt before the Smith, who fixed things that were broken, \\nand asked that he give her sweet Bran his protection.She went to the Maid and beseeched her to lend her courage to Arya and Sansa, \\nto guard them in their innocence.To the Father, she prayed for justice, the strength to seek it and the wisdom to know it, \\nand she asked the Warrior to keep Robb strong and shield him in his battles.Lastly she turned to the Crone, whose statues often showed her with a lamp in one hand.\"Guide me, wise lady,\" she prayed.\"Show me the path I must walk, and do not let me stumble in the dark places that lie ahead.\"'"]},"execution_count":19,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["## with space=True\n","\"\".join([\"\".join(sentence) for sentence in word_sentence_tokenize(text, True)])"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"executionInfo":{"elapsed":427,"status":"ok","timestamp":1610801792687,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"0gdRqc9SQiDF","outputId":"eb7d35c0-ed49-483e-c93e-c607f26cc6a7"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Lost and weary , Catelyn Stark gave herself over to her gods . She knelt before the Smith , who fixed things that were broken , \\n and asked that he give her sweet Bran his protection . She went to the Maid and beseeched her to lend her courage to Arya and Sansa , \\n to guard them in their innocence . To the Father , she prayed for justice , the strength to seek it and the wisdom to know it , \\n and she asked the Warrior to keep Robb strong and shield him in his battles . Lastly she turned to the Crone , whose statues often showed her with a lamp in one hand . \" Guide me , wise lady , \" she prayed . \" Show me the path I must walk , and do not let me stumble in the dark places that lie ahead . \"'"]},"execution_count":20,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["## with space=False\n","\" \".join([\" \".join(sentence) for sentence in word_sentence_tokenize(text, False)])"]},{"cell_type":"markdown","metadata":{"id":"NW3Uk7gTQiDF"},"source":["#### A.4 Write a function that loads/processes a document from file\n","Write a function called `load_data(path, space = False)` which accepts a `path` string to identify the direct location of a text file. Upon loading the specified file, construct an (output) dictionary called `data` with three key-value pairs:\n","\n","- `'sentences'`: output of word_sentence_tokenize applied to document,\n","- `'counts'`: a dictionary of integer counts of all tokens in the document,\n","- `'type_index'`: a dictionary linking tokens to indices for their order of appearance.\n","\n","Test this code on the books in the local `'./data/books/'` directory, e.g., `'./data/books/84.txt'` is a copy of \"Frankenstein...\" (other metadata can be found in `'./data/books/metadata.json'`)."]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":453,"status":"ok","timestamp":1610801795321,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"qMCkO-kHuqE_"},"outputs":[],"source":["def load_data(path, space = False):\n","    data = {'sentences': word_sentence_tokenize(open(path).read().strip().lower(), space)}\n","    data['counts'] = dict(Counter([t for s in data['sentences'] for t in s]))\n","    data['type_index'] = {t: i for i, t in enumerate(data['counts'])}\n","    return data"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":994,"status":"ok","timestamp":1610801797364,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"EqCblNv_QiDG"},"outputs":[],"source":["data = load_data('./data/books/84.txt', True)"]},{"cell_type":"markdown","metadata":{"id":"N6L-tsbuQiDG"},"source":["#### A.5 Build a context generator\n","Now write a function called `get_context(i, sentence, m = 0, weight = 0)` to produce a 'sliding-window' context (list of surrounding tokens) for the token of index `i` in an already tokenized `sentence` (a list of strings). Optional non-negative arguments `m` (an integer) and `weight` (a float) specify the size of the context window and the relative weights of context elements.\n","\n","Specifically, `m` tokens should be taken to both the left and right of token `i` (all should be taken when the default `m=0` is set. \n","\n","Finally, `weight` should determine how to return in a list named `weights`, which should be numeric and of length equal to that of the `context`. The contents of `weights` should be the reciprocal of the absolute distance to the center token, i.e., the token of index `i`---_raised to the power valued by `weight`_. Note: this ensures setting `weight=0` 'turns off' the weights."]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":644,"status":"ok","timestamp":1610801799514,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"WT3AXQn5IPMA"},"outputs":[],"source":["def get_context(i, sentence, m = 0, weight = 0):\n","    context = np.array(sentence)\n","    weights = np.abs(np.array(range(len(sentence))) - i)\n","    if m:\n","        mask = (weights != 0) & (weights <= m)\n","    else:\n","        mask = (weights != 0)\n","    context = context[mask]\n","    if weight:\n","        weights = 1/(weights[mask]**weight)\n","    else:\n","        weights = weights[mask]*weight + 1.\n","    return context, weights"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":302,"status":"ok","timestamp":1610801800337,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"_2S0N9khQiDG","outputId":"ed487603-c858-42db-d50b-90f6599bf552"},"outputs":[{"data":{"text/plain":["(array(['This', 'is', 'an', 'example', 'the', 'space', 'flag', 'as',\n","        'False', '!'], dtype='<U7'),\n"," array([0.25      , 0.33333333, 0.5       , 1.        , 1.        ,\n","        0.5       , 0.33333333, 0.25      , 0.2       , 0.16666667]))"]},"execution_count":24,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["get_context(4, tokenize('This is an example with the space flag as False!', False), 0, 1)"]},{"cell_type":"markdown","metadata":{"id":"Pmj_mz9PQiDH"},"source":["#### A.6 Compute a co-occurrence matrix\n","Finally, we'll utilize our context model and two-stage tokenizer to build a co-occurrence matrix with weighted contexts.\n","\n","In particular, build a function called `compute_co_occurrence_matrix(data, m = 0, weight = 0)` that accepts the `data` output from `load_data()` and constructs `X`&mdash;an `N` (the vocabulary size) by `N` matrix with each row (token) and column (context) corresponding to the _total `weight`_ in which context tokens appear in the `m`-context windows of 'center' tokens. \n","\n","Note: the rows and columns of `X` should be in the order specified by `data['type_index']`."]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":399,"status":"ok","timestamp":1610801803496,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"BQUO5itB3Sjh"},"outputs":[],"source":["def compute_co_occurrence_matrix(data, m = 0, weight = 0):\n","    N = len(data['type_index'])\n","    X = np.zeros((N, N))    \n","    for sentence in data['sentences']:\n","        for i, t in enumerate(sentence):\n","            context, weights = get_context(i, sentence, m, weight)            \n","            for c, w in zip(context, weights):\n","                X[data['type_index'][t],data['type_index'][c]] += w\n","    return X"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":16569,"status":"ok","timestamp":1610801820849,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"hR6iWTflQiDK"},"outputs":[],"source":["X = compute_co_occurrence_matrix(data, m = 0, weight = 1)"]},{"cell_type":"markdown","metadata":{"id":"XkLXv6UYQiDK"},"source":["#### A.7 Build a similarity function to sanity check our model\n","Here, we should build a cosine-similarity comparer: `most_similar(t, type_index, X, top=10)` that accepts a token `t` and the `type_index` (from `data['type_index']`), the latter of which should link any string to the rows/columns of `X`. The final arguemnt `top` specifies how many results the function should produce in output. Finally, this output should (as in Chapter 1) consist of a sorted (high-to-low, by similarity) list of `(token, similarity)` tuples."]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":871,"status":"ok","timestamp":1610801826017,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"DMUddofLQiDL","outputId":"387a78d9-2232-4b57-8ae9-207a4ba27be8"},"outputs":[{"data":{"text/plain":["[('\\n', 0.9979221135186093),\n"," ('night', 0.9964860035918699),\n"," ('youth', 0.9963653051872496),\n"," ('rest', 0.996178645914909),\n"," ('conversation', 0.9961603312382916),\n"," ('light', 0.9961374186551338),\n"," ('murder', 0.9960937314047589),\n"," ('work', 0.9960868608188356),\n"," ('town', 0.9960474824393188)]"]},"execution_count":27,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["def most_similar(t, type_index, X, top=10):\n","    vec = X / np.linalg.norm(X, axis=1)[:, np.newaxis]\n","    v = vec[type_index[t],:]\n","    similar = sorted(enumerate(list(vec.dot(v))), \n","                     key = lambda x: x[1], reverse = True)\n","    types = list(type_index.keys())\n","    if not top: top = len(vec.shape[0])\n","    sims = [(types[ix], sim) for ix, sim in similar[:top] if ix != type_index[t]]\n","    return sims\n","most_similar(' ', data['type_index'], X, top=10)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":818,"status":"ok","timestamp":1610801827673,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"rj1g6ZD6QiDL","outputId":"54825bdc-cfdb-4704-8cbc-cb6f5497b0d9"},"outputs":[{"data":{"text/plain":["[(',', 0.9987401007903789),\n"," (';', 0.9983632870995872),\n"," ('from', 0.9979362111260544),\n"," ('with', 0.9978875525838681),\n"," ('in', 0.9978153315451614),\n"," ('gave', 0.9977828700178513),\n"," ('made', 0.9977621623725164),\n"," ('the', 0.9977579179934447),\n"," ('every', 0.997696092191149)]"]},"execution_count":28,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["most_similar('.', data['type_index'], X, top=10)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":947,"status":"ok","timestamp":1610801829227,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"ecZQBj9cQiDM","outputId":"01bdf5d3-613d-4c51-9c66-8cb63a1d11ea"},"outputs":[{"data":{"text/plain":["[('he', 0.9992915660314543),\n"," ('i', 0.9992414557834342),\n"," ('it', 0.9989050254483557),\n"," ('her', 0.998874469254197),\n"," ('who', 0.9987882861329731),\n"," ('very', 0.9987420408496845),\n"," ('so', 0.9987201503754392),\n"," ('all', 0.9987188616943543),\n"," ('and', 0.9987021137096252)]"]},"execution_count":29,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["most_similar('she', data['type_index'], X, top=10)"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":678,"status":"ok","timestamp":1610801830840,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"9dpZaGyLQiDM","outputId":"db849a12-c3c0-46a8-9762-3932b88e2ba7"},"outputs":[{"data":{"text/plain":["[('i', 0.9996188111294829),\n"," ('she', 0.9992915660314543),\n"," ('so', 0.9992129104853836),\n"," ('and', 0.9991085711836337),\n"," ('all', 0.9990503074413479),\n"," ('who', 0.999023137972763),\n"," ('this', 0.9990098627247053),\n"," ('we', 0.9989412910775652),\n"," ('or', 0.9989385808026763)]"]},"execution_count":30,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["most_similar('he', data['type_index'], X, top=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":57475,"status":"aborted","timestamp":1610801739076,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"VSYz4zzhQiDT"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"exercises-solutions.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.11.0 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"vscode":{"interpreter":{"hash":"5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"}}},"nbformat":4,"nbformat_minor":0}
