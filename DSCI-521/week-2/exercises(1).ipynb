{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52873,"status":"ok","timestamp":1610801734298,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"JpVv_wqkQleX","outputId":"38b06896-b84c-4d9d-80ae-2e38e2e95348"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","nbdir = \"/content/gdrive/My Drive/DSCI521/Colab/02-textual/\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52869,"status":"ok","timestamp":1610801734301,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"FBOjRlBgQqjY","outputId":"4adb847f-c83f-455a-858e-50d1145eac3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/gdrive/My Drive/DSCI521/Colab/02-textual\n"]}],"source":["%cd /content/gdrive/My\\ Drive/DSCI521/Colab/02-textual/"]},{"cell_type":"markdown","metadata":{"id":"iN0D_7g6t64v"},"source":["# DSCI 521: Methods for analysis and interpretation <br>Chapter 2: Feature engineering and language processing\n","\n","## Exercises\n","Note: numberings refer to the main notes."]},{"cell_type":"markdown","metadata":{"id":"7FfYvUBut641"},"source":["#### 2.1.1.3 Exercise: Regex phone numbers\n","Read the file `phone-numbers.txt`. It contains a phone number in each line. \\[Hint: use something like `lines = open(\"file.txt\", \"r\").readlines()`\\] Store only the phone numbers with the area code \"215\" in a list and print it out. Use regex-based pattern matching, not any other methods which occur to you."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"mki4ZNE1RoJr"},"outputs":[{"name":"stdout","output_type":"stream","text":["215-345-3463\n","\n","215-756-8273\n","\n"]}],"source":["## code here\n","\n","lines = open(\"data/phone-numbers.txt\").readlines()\n","import re \n","## phone numbers start with 215\n","pattern = re.compile(r\"^215\")\n","for line in lines:\n","    if pattern.search(line):\n","        print(line)\n","    "]},{"cell_type":"markdown","metadata":{"id":"-bY1gFqlt644"},"source":["#### 2.1.1.8 Exercise: Names of the gods\n","In the cell below is some text. It's an extract from [A Clash of Kings](https://www.goodreads.com/book/show/10572.A_Clash_of_Kings), specifically, about a character's prayer to some fictional gods. Use regex to extract the names of these gods. Your output should be a list that looks something like `[\"the Father\", \"the Mother\", \"the Warrior\"]`."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"QJ1Rj2HhSvpv"},"outputs":[{"name":"stdout","output_type":"stream","text":["['the Smith', 'the Maid', 'the Father', 'the Warrior', 'the Crone']\n"]}],"source":["text = 'Lost and weary, Catelyn Stark gave herself over to her gods. She knelt before the Smith, who fixed things that were broken, and asked that he give her sweet Bran his protection. She went to the Maid and beseeched her to lend her courage to Arya and Sansa, to guard them in their innocence. To the Father, she prayed for justice, the strength to seek it and the wisdom to know it, and she asked the Warrior to keep Robb strong and shield him in his battles. Lastly she turned to the Crone, whose statues often showed her with a lamp in one hand. \"Guide me, wise lady,\" she prayed. \"Show me the path I must walk, and do not let me stumble in the dark places that lie ahead.\"'\n","\n","## code here\n","## using regex to find words that start with the \n","pattern = re.findall(\"the [A-Z][a-z]+\",text)\n","print(pattern)"]},{"cell_type":"markdown","metadata":{"id":"fdWSdF77t645"},"source":["#### 2.1.2.4 Exercise: Improving a regex-based sentence tokenizer\n","First, write a few sentences in a complex (but grammatically acceptable) way so that the (above) regex-based tokenizer breaks. Then, fix the pattern so that the tokenizer can handle your text appropriately."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"su3YCIsxRxKh"},"outputs":[],"source":["## code here\n"]},{"cell_type":"markdown","metadata":{"id":"GkH2EXn1t646"},"source":["#### 2.1.3.2 Exercise: POS tagging \n","Apply POS tagging to a sentence of your choosing and filter for only verbs and nouns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tbTOJd3_R1UC"},"outputs":[],"source":["## code here"]},{"cell_type":"markdown","metadata":{"id":"Wv3N3rjct647"},"source":["#### 2.1.3.5 Exercise: using grammar for information extraction\n","Apply the spacy grammatical parsing and extract any subject-verb token pairs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f-89fuUgR7Np"},"outputs":[],"source":["## code here"]},{"cell_type":"markdown","metadata":{"id":"ur3A5wbRt649"},"source":["#### 2.1.4.4 Exercise: improved word frequency representation\n","Build a stop word list and lemmatization strategy (potentially using POS tags) to compute 'better' word frequencies, as you see fit."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"naP-mz5FSKYP"},"outputs":[],"source":["text = \"\"\"Word frequencies are probably the first and easiest \n","numerical representation of text to compute. In some communities, \n","this is referred to as the bag of words (BOW) model. \n","Put simply, the BOW model simply counts up the \n","number of times each word appears in a document. \n","This of course depends on a few things, e.g., case and lemmatization. \n","However, constructing a basic BOW model is quite straightforward, especially using `Counter`. \n","Let's use this very paragraph as our example text for the BOW model.\"\"\"\n","## code here"]},{"cell_type":"markdown","metadata":{"id":"bX49XzwWt64-"},"source":["#### 2.1.6.5 Exercise: exploring TF-IDF\n","Rank each of the example TF-IDF matrix's rows by TF-IDF values from high-to-low and interpret the kinds of words that have high TF-IDF values, i.e., are 'more important'. What about the low values, what kinds of words are these?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ZPZUWbxSRAg"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZjm2OHyt64-"},"outputs":[],"source":["import numpy as np\n","\n","def count_words(sentence):\n","    frequency = Counter()\n","    for word in sentence:\n","        frequency[word.text.lower()] += 1\n","    return frequency\n","\n","text = '''Lost and weary, Catelyn Stark gave herself over to her gods. \n","She knelt before the Smith, who fixed things that were broken, \n","and asked that he give her sweet Bran his protection. \n","She went to the Maid and beseeched her to lend her courage to Arya and Sansa, \n","to guard them in their innocence. \n","To the Father, she prayed for justice, the strength to seek it and the wisdom to know it, \n","and she asked the Warrior to keep Robb strong and shield him in his battles. \n","Lastly she turned to the Crone, whose statues often showed her with a lamp in one hand. \n","\"Guide me, wise lady,\" she prayed. \n","\"Show me the path I must walk, and do not let me stumble in the dark places that lie ahead.\"\n","'''\n","\n","doc = nlp(text)\n","    \n","## the 'master' set, keeps track of the words in all documents\n","all_words = set()\n","\n","## store the word frequencies by book\n","all_doc_frequencies = {}\n","\n","## loop over the sentences\n","for j, sentence in enumerate(doc.sents):\n","    frequency = count_words(sentence)\n","    all_doc_frequencies[j] = frequency\n","    doc_words = set(frequency.keys())\n","    all_words = all_words.union(doc_words)\n","    \n","## create a matrix of zeros: (words) x (documents)\n","TDM = np.zeros((len(all_words),len(all_doc_frequencies)))\n","## fix a word ordering for the rows\n","all_words = sorted(list(all_words))\n","## loop over the (sorted) document numbers and (ordered) words; fill in matrix\n","for j in all_doc_frequencies:\n","    for i, word in enumerate(all_words):\n","        TDM[i,j] = all_doc_frequencies[j][word]\n","\n","num_docs = TDM.shape[1]\n","\n","## start off with a copy of our TDM (frequencies)\n","TFIDF = np.array(TDM)\n","## loop over words\n","for i, word in enumerate(all_words):\n","    ## count docs containing the word\n","    num_docs_containing_word = len([x for x in TDM[i] if x])\n","    ### computen the inverse document frequence of this word\n","    IDF = -np.log2(num_docs_containing_word/num_docs)\n","    ## multiply this row by the IDF to transform it to TFIDF\n","    TFIDF[i,] = TFIDF[i,]*IDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PuHp1IeCSmXl"},"outputs":[],"source":["## code here"]},{"cell_type":"markdown","metadata":{"id":"_-1p47ipglpg"},"source":["## Additional In-depth Exercises\n","\n","### A. Constructing co-occurrence matrix statistics\n","\n","#### A.1 Build a tokenizer\n","To start, build a tokenization function called `tokens = tokenize(text, space = False)` that accepts a string called `text`, in addition to a boolean argument called `space`, which if positive will allow the tokenize function to determine if whitespace characters (at all) should be stored as a part of the list of `tokens` output.\n","\n","For this part of the exercise, use the character-class `'[0-9a-zA-Z'-]'` (or it's complimentary character class) to split on non-delimiters, but be sure to capture all portions of the text that are 'split' using a grouping mechanism. Likewise, ensure that all non-word-type tokens are completely resolved, e.g., there _shouldn't_ be any tokens which consist of multiple punctuation characters, such as `\".\\\"\"`, which should be sub-divided into multiple tokens.\n","\n","Likewise, be sure to collapse any multiple whitespace `\" \"` characters down to just one as an initial pre-processing step to the `text`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yphIXwtMS35r"},"outputs":[],"source":["## code here"]},{"cell_type":"markdown","metadata":{"id":"YSQ7l-D6QiDB"},"source":["#### A.2 Build a word-sentence tokenizer\n","Here, the goal will be to produce a two-level tokenization utility that is similar to what Spacy produces:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55380,"status":"ok","timestamp":1610801736907,"user":{"displayName":"Dr. Jake","photoUrl":"","userId":"10996578946780976051"},"user_tz":300},"id":"rZTPpPDqYLtV","outputId":"725d3d56-af06-4a90-b608-27ca6e5db5d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Lost', 'and', 'weary', ',', 'Catelyn', 'Stark', 'gave', 'herself', 'over', 'to', 'her', 'gods', '.', '\\n']\n","['She', 'knelt', 'before', 'the', 'Smith', ',', 'who', 'fixed', 'things', 'that', 'were', 'broken', ',', '\\n', 'and', 'asked', 'that', 'he', 'give', 'her', 'sweet', 'Bran', 'his', 'protection', '.', '\\n']\n","['She', 'went', 'to', 'the', 'Maid', 'and', 'beseeched', 'her', 'to', 'lend', 'her', 'courage', 'to', 'Arya', 'and', 'Sansa', ',', '\\n', 'to', 'guard', 'them', 'in', 'their', 'innocence', '.', '\\n']\n","['To', 'the', 'Father', ',', 'she', 'prayed', 'for', 'justice', ',', 'the', 'strength', 'to', 'seek', 'it', 'and', 'the', 'wisdom', 'to', 'know', 'it', ',', '\\n', 'and', 'she', 'asked', 'the', 'Warrior', 'to', 'keep', 'Robb', 'strong', 'and', 'shield', 'him', 'in', 'his', 'battles', '.', '\\n']\n","['Lastly', 'she', 'turned', 'to', 'the', 'Crone', ',', 'whose', 'statues', 'often', 'showed', 'her', 'with', 'a', 'lamp', 'in', 'one', 'hand', '.', '\\n']\n","['\"', 'Guide', 'me', ',', 'wise', 'lady', ',', '\"', 'she', 'prayed', '.', '\\n']\n","['\"', 'Show', 'me', 'the', 'path', 'I', 'must', 'walk', ',', 'and', 'do', 'not', 'let', 'me', 'stumble', 'in', 'the', 'dark', 'places', 'that', 'lie', 'ahead', '.', '\"', '\\n']\n"]}],"source":["for s in doc.sents:\n","    print([w.text for w in s])"]},{"cell_type":"markdown","metadata":{"id":"jaDdM_w7QiDC"},"source":["with the caveat that we use our own tokenization utility (which can be flagged to retain space characters).\n","Since this will then require the utilization of a sentence tokenizer, download `nltk` (if you haven't already) and utilize its `sent_tokenize()` function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pM7YjkrWTBnB"},"outputs":[],"source":["## code here"]},{"cell_type":"markdown","metadata":{"id":"_Zzh8vtXQiDD"},"source":["#### A.3 Try to re-construct the document\n","Now that we have the two-stage tokenizer which can retain space characters, let's try an re-construct a document from its tokenization, with and without `space=True`.\n","\n","In particular, consider how to re-join the elements of the two-level list (sentences) of lists (words) of strings by a delimiter so as to re-construct the document."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-jAnilxeTKYx"},"outputs":[],"source":["## code here with space=True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjRCfEfeTMYX"},"outputs":[],"source":["## code here with space=False"]},{"cell_type":"markdown","metadata":{"id":"NW3Uk7gTQiDF"},"source":["#### A.4 Write a function that loads/processes a document from file\n","Write a function called `load_data(path, space = False)` which accepts a `path` string to identify the direct location of a text file. Upon loading the specified file, construct an (output) dictionary called `data` with three key-value pairs:\n","\n","- `'sentences'`: output of word_sentence_tokenize applied to document,\n","- `'counts'`: a dictionary of integer counts of all tokens in the document,\n","- `'type_index'`: a dictionary linking tokens to indices for their order of appearance.\n","\n","Test this code on the books in the local `'./data/books/'` directory, e.g., `'./data/books/84.txt'` is a copy of \"Frankenstein...\" (other metadata can be found in `'./data/books/metadata.json'`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8e6ZS24yTRlz"},"outputs":[],"source":["## code here"]},{"cell_type":"markdown","metadata":{"id":"N6L-tsbuQiDG"},"source":["#### A.5 Build a context generator\n","Now write a function called `get_context(i, sentence, m = 0, weight = 0)` to produce a 'sliding-window' context (list of surrounding tokens) for the token of index `i` in an already tokenized `sentence` (a list of strings). Optional non-negative arguments `m` (an integer) and `weight` (a float) specify the size of the context window and the relative weights of context elements.\n","\n","Specifically, `m` tokens should be taken to both the left and right of token `i` (all should be taken when the default `m=0` is set. \n","\n","Finally, `weight` should determine how to return in a list named `weights`, which should be numeric and of length equal to that of the `context`. The contents of `weights` should be the reciprocal of the absolute distance to the center token, i.e., the token of index `i`---_raised to the power valued by `weight`_. Note: this ensures setting `weight=0` 'turns off' the weights."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XsdjsEo4TTkr"},"outputs":[],"source":["## code here"]},{"cell_type":"markdown","metadata":{"id":"Pmj_mz9PQiDH"},"source":["#### A.6 Compute a co-occurrence matrix\n","Finally, we'll utilize our context model and two-stage tokenizer to build a co-occurrence matrix with weighted contexts.\n","\n","In particular, build a function called `compute_co_occurrence_matrix(data, m = 0, weight = 0)` that accepts the `data` output from `load_data()` and constructs `X`&mdash;an `N` (the vocabulary size) by `N` matrix with each row (token) and column (context) corresponding to the _total `weight`_ in which context tokens appear in the `m`-context windows of 'center' tokens. \n","\n","Note: the rows and columns of `X` should be in the order specified by `data['type_index']`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fkOl9PtJTWT4"},"outputs":[],"source":["## code here"]},{"cell_type":"markdown","metadata":{"id":"XkLXv6UYQiDK"},"source":["#### A.7 Build a similarity function to sanity check our model\n","Here, we should build a cosine-similarity comparer: `most_similar(t, type_index, X, top=10)` that accepts a token `t` and the `type_index` (from `data['type_index']`), the latter of which should link any string to the rows/columns of `X`. The final arguemnt `top` specifies how many results the function should produce in output. Finally, this output should (as in Chapter 1) consist of a sorted (high-to-low, by similarity) list of `(token, similarity)` tuples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WFJMPSx6Td6R"},"outputs":[],"source":["## code here"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"exercises.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.10.8 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}}},"nbformat":4,"nbformat_minor":0}
