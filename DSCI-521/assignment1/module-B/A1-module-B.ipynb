{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module submission header\n",
    "### Submission preparation instructions \n",
    "_Completion of this header is mandatory, subject to a 2-point deduction to the assignment._ Only add plain text in the designated areas, i.e., replacing the relevant 'NA's. You must fill out all group member Names and Drexel email addresses in the below markdown list, under header __Module submission group__. It is required to fill out descriptive notes pertaining to any tutoring support received in the completion of this submission under the __Additional submission comments__ section at the bottom of the header. If no tutoring support was received, leave NA in place. You may as well list other optional comments pertaining to the submission at bottom. _Any distruption of this header's formatting will make your group liable to the 2-point deduction._\n",
    "\n",
    "### Module submission group\n",
    "- Group member 1\n",
    "    - Name: Rishabh Sharma\n",
    "    - Email: rs3738@drexel.edu\n",
    "- Group member 2\n",
    "    - Name: Shai Wudkwych\n",
    "    - Email: sw3468@drexel.edu\n",
    "\n",
    "### Additional submission comments\n",
    "- Tutoring support received: NA\n",
    "- Other (other): NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment group 1: Textual feature extraction and numerical comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module B _(35 points)_ Key word in context\n",
    "\n",
    "Key word in context (KWiC) is a common format for concordance lines, i.e., contextualized instances of principal words used in a book. More generally, KWiC is essentially the concept behind the utility of 'find in page' on document viewers and web browsers. This module builds up a KWiC utility for finding key word-containing sentences, and 'most relevant' paragraphs, quickly.\n",
    "\n",
    "__B1.__ _(3 points)_ Start by writing a function called `load_book` that reads in a book based on a provided `book_id` string and returns a list of `paragraphs` from the book. When book data is loaded, you should remove the space characters at the beginning and end of the text (e.g., using `strip()`). Then, to split books into paragraphs, use the `re.split()` method to split the input in cases where there are two or more new lines. Note, that books are in the provided `data/books` directory.\n",
    "\n",
    "Note: this module is not focused on text pre-processing beyond a split into paragraphs; you do _not_ need to remove markup or non-substantive content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/rishabhsharma/Documents/GitHub/winter-23/DSCI-521/assignment1/module-B/data/books/84.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1:Function(3/3)\n",
    "\n",
    "import re\n",
    "\n",
    "def load_book(book_id):\n",
    "    #---Your code start here---\n",
    "    book = './data/books/'+book_id+'.txt'\n",
    "    with open(book, 'r') as f:\n",
    "        text = f.read()\n",
    "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "    #---Your code ends here---\n",
    "    \n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your function, lets apply it to look at a few paragraphs from book 84."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "725\n",
      "I am already far north of London, and as I walk in the streets of\n",
      "Petersburgh, I feel a cold northern breeze play upon my cheeks, which\n",
      "braces my nerves and fills me with delight.  Do you understand this\n",
      "feeling?  This breeze, which has travelled from the regions towards\n",
      "which I am advancing, gives me a foretaste of those icy climes.\n",
      "Inspirited by this wind of promise, my daydreams become more fervent\n",
      "and vivid.  I try in vain to be persuaded that the pole is the seat of\n",
      "frost and desolation; it ever presents itself to my imagination as the\n",
      "region of beauty and delight.  There, Margaret, the sun is forever\n",
      "visible, its broad disk just skirting the horizon and diffusing a\n",
      "perpetual splendour.  There--for with your leave, my sister, I will put\n",
      "some trust in preceding navigators--there snow and frost are banished;\n",
      "and, sailing over a calm sea, we may be wafted to a land surpassing in\n",
      "wonders and in beauty every region hitherto discovered on the habitable\n",
      "globe.  Its productions and features may be without example, as the\n",
      "phenomena of the heavenly bodies undoubtedly are in those undiscovered\n",
      "solitudes.  What may not be expected in a country of eternal light?  I\n",
      "may there discover the wondrous power which attracts the needle and may\n",
      "regulate a thousand celestial observations that require only this\n",
      "voyage to render their seeming eccentricities consistent forever.  I\n",
      "shall satiate my ardent curiosity with the sight of a part of the world\n",
      "never before visited, and may tread a land never before imprinted by\n",
      "the foot of man. These are my enticements, and they are sufficient to\n",
      "conquer all fear of danger or death and to induce me to commence this\n",
      "laborious voyage with the joy a child feels when he embarks in a little\n",
      "boat, with his holiday mates, on an expedition of discovery up his\n",
      "native river. But supposing all these conjectures to be false, you\n",
      "cannot contest the inestimable benefit which I shall confer on all\n",
      "mankind, to the last generation, by discovering a passage near the pole\n",
      "to those countries, to reach which at present so many months are\n",
      "requisite; or by ascertaining the secret of the magnet, which, if at\n",
      "all possible, can only be effected by an undertaking such as mine.\n"
     ]
    }
   ],
   "source": [
    "# B1:SanityCheck\n",
    "paragraphs = load_book('84')\n",
    "print(len(paragraphs))\n",
    "print(paragraphs[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B2.__ _(10 points)_ Next, write a function called `kwic(paragraphs, search_terms)` that accepts a list of string `paragraphs` and a set of `search_term` strings. The function should:\n",
    "\n",
    "1. initialize `data` as a `defaultdict` of lists\n",
    "2. loop over the `paragraphs` and apply `spacy`'s processing to produce a `doc` for each;\n",
    "3. loop over the `doc.sents` resulting from each `paragraph`;\n",
    "4. loop over the words in each `sentence`;\n",
    "5. check: `if` a `word` is `in` the `search_terms` set;\n",
    "6. `if` (5), then `.append()` the reference under `data[word]` as a list: `[[i, j, k], sentence]`, where `i`, `j`, and `k` refer to the paragraph-in-book, sentence-in-paragraph, and word-in-sentence indices, respectively.\n",
    "\n",
    "Your output, `data`, should then be a default dictionary of lists of the format:\n",
    "```\n",
    "data['word'] = [[[i, j, k], [\"These\", \"are\", \"sentences\", \"containing\", \"the\", \"word\", \"'word'\", \".\"]],\n",
    "                ...,]\n",
    "```\n",
    "\n",
    "Note, we have imported spacy and set it up to use the `\"en\"` model. This will require you to install spacy by running `pip install spacy` and downloading the `\"en\"` model by running the command `python -m spacy download en`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B2:Function(10/10)\n",
    "\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def kwic(paragraphs, search_terms = {}):\n",
    "    #---Your code starts here---\n",
    "    data =defaultdict(list)\n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        doc = nlp(paragraph)\n",
    "        for j, sentence in enumerate(doc.sents):\n",
    "            for k, word in enumerate(sentence):\n",
    "                if word.text in search_terms:\n",
    "                    data[word.text].append([[i, j, k], [token.text for token in sentence]])\n",
    "    #---Your code ends here---\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test your function using the paragraphs from your `load_book` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'Ocean': [[[11, 2, 26],\n",
       "               ['I',\n",
       "                '\\n',\n",
       "                'have',\n",
       "                'read',\n",
       "                'with',\n",
       "                'ardour',\n",
       "                'the',\n",
       "                'accounts',\n",
       "                'of',\n",
       "                'the',\n",
       "                'various',\n",
       "                'voyages',\n",
       "                'which',\n",
       "                'have',\n",
       "                '\\n',\n",
       "                'been',\n",
       "                'made',\n",
       "                'in',\n",
       "                'the',\n",
       "                'prospect',\n",
       "                'of',\n",
       "                'arriving',\n",
       "                'at',\n",
       "                'the',\n",
       "                'North',\n",
       "                'Pacific',\n",
       "                'Ocean',\n",
       "                '\\n',\n",
       "                'through',\n",
       "                'the',\n",
       "                'seas',\n",
       "                'which',\n",
       "                'surround',\n",
       "                'the',\n",
       "                'pole',\n",
       "                '.',\n",
       "                ' ']]],\n",
       "             'seas': [[[11, 2, 30],\n",
       "               ['I',\n",
       "                '\\n',\n",
       "                'have',\n",
       "                'read',\n",
       "                'with',\n",
       "                'ardour',\n",
       "                'the',\n",
       "                'accounts',\n",
       "                'of',\n",
       "                'the',\n",
       "                'various',\n",
       "                'voyages',\n",
       "                'which',\n",
       "                'have',\n",
       "                '\\n',\n",
       "                'been',\n",
       "                'made',\n",
       "                'in',\n",
       "                'the',\n",
       "                'prospect',\n",
       "                'of',\n",
       "                'arriving',\n",
       "                'at',\n",
       "                'the',\n",
       "                'North',\n",
       "                'Pacific',\n",
       "                'Ocean',\n",
       "                '\\n',\n",
       "                'through',\n",
       "                'the',\n",
       "                'seas',\n",
       "                'which',\n",
       "                'surround',\n",
       "                'the',\n",
       "                'pole',\n",
       "                '.',\n",
       "                ' ']],\n",
       "              [[22, 8, 10],\n",
       "               ['Shall',\n",
       "                'I',\n",
       "                'meet',\n",
       "                'you',\n",
       "                'again',\n",
       "                ',',\n",
       "                'after',\n",
       "                'having',\n",
       "                'traversed',\n",
       "                'immense',\n",
       "                'seas',\n",
       "                ',',\n",
       "                'and',\n",
       "                '\\n',\n",
       "                'returned',\n",
       "                'by',\n",
       "                'the',\n",
       "                'most',\n",
       "                'southern',\n",
       "                'cape',\n",
       "                'of',\n",
       "                'Africa',\n",
       "                'or',\n",
       "                'America',\n",
       "                '?',\n",
       "                ' ']],\n",
       "              [[31, 2, 14],\n",
       "               ['Thus',\n",
       "                'far',\n",
       "                'I',\n",
       "                'have',\n",
       "                '\\n',\n",
       "                'gone',\n",
       "                ',',\n",
       "                'tracing',\n",
       "                'a',\n",
       "                'secure',\n",
       "                'way',\n",
       "                'over',\n",
       "                'the',\n",
       "                'pathless',\n",
       "                'seas',\n",
       "                ',',\n",
       "                'the',\n",
       "                'very',\n",
       "                'stars',\n",
       "                '\\n',\n",
       "                'themselves',\n",
       "                'being',\n",
       "                'witnesses',\n",
       "                'and',\n",
       "                'testimonies',\n",
       "                'of',\n",
       "                'my',\n",
       "                'triumph',\n",
       "                '.',\n",
       "                ' ']],\n",
       "              [[379, 4, 21],\n",
       "               ['I',\n",
       "                'had',\n",
       "                'a',\n",
       "                'very',\n",
       "                '\\n',\n",
       "                'confused',\n",
       "                'knowledge',\n",
       "                'of',\n",
       "                'kingdoms',\n",
       "                ',',\n",
       "                'wide',\n",
       "                'extents',\n",
       "                'of',\n",
       "                'country',\n",
       "                ',',\n",
       "                'mighty',\n",
       "                'rivers',\n",
       "                ',',\n",
       "                '\\n',\n",
       "                'and',\n",
       "                'boundless',\n",
       "                'seas',\n",
       "                '.',\n",
       "                ' ']],\n",
       "              [[669, 8, 20],\n",
       "               ['I',\n",
       "                'had',\n",
       "                'determined',\n",
       "                ',',\n",
       "                'if',\n",
       "                'you',\n",
       "                'were',\n",
       "                'going',\n",
       "                'southwards',\n",
       "                ',',\n",
       "                'still',\n",
       "                'to',\n",
       "                'trust',\n",
       "                '\\n',\n",
       "                'myself',\n",
       "                'to',\n",
       "                'the',\n",
       "                'mercy',\n",
       "                'of',\n",
       "                'the',\n",
       "                'seas',\n",
       "                'rather',\n",
       "                'than',\n",
       "                'abandon',\n",
       "                'my',\n",
       "                'purpose',\n",
       "                '.',\n",
       "                ' ']],\n",
       "              [[677, 15, 5],\n",
       "               ['Behold',\n",
       "                ',',\n",
       "                'on',\n",
       "                'these',\n",
       "                'desert',\n",
       "                'seas',\n",
       "                'I',\n",
       "                'have',\n",
       "                '\\n',\n",
       "                'found',\n",
       "                'such',\n",
       "                'a',\n",
       "                'one',\n",
       "                ',',\n",
       "                'but',\n",
       "                'I',\n",
       "                'fear',\n",
       "                'I',\n",
       "                'have',\n",
       "                'gained',\n",
       "                'him',\n",
       "                'only',\n",
       "                'to',\n",
       "                'know',\n",
       "                'his',\n",
       "                'value',\n",
       "                '\\n',\n",
       "                'and',\n",
       "                'lose',\n",
       "                'him',\n",
       "                '.',\n",
       "                ' ']]]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B2:SanityCheck\n",
    "kwic(paragraphs, {'Ocean', 'seas'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B3.__ _(2 points)_ Let's test your `kwic` search function's utility using the pre-processed `paragraphs` from book `84` for the key words `Frankenstein` and `monster` in context. Answer the inline questions about these tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences 'Frankenstein' appears in: 27\n",
      "# of sentences 'monster' appears in: 31\n",
      "\n",
      "Frankenstein ,\n",
      "\n",
      "I started from my sleep with horror ; a cold dew covered my forehead , my \n",
      " teeth chattered , and every limb became convulsed ; when , by the dim and \n",
      " yellow light of the moon , as it forced its way through the window \n",
      " shutters , I beheld the wretch -- the miserable monster whom I had \n",
      " created .  \n"
     ]
    }
   ],
   "source": [
    "# B3:SanityCheck\n",
    "results = kwic(paragraphs, {\"Frankenstein\", \"monster\"})\n",
    "\n",
    "print(\"# of sentences 'Frankenstein' appears in: {}\".format(len(results['Frankenstein'])))\n",
    "print(\"# of sentences 'monster' appears in: {}\".format(len(results['monster'])))\n",
    "print()\n",
    "\n",
    "print(\" \".join(results['Frankenstein'][0][1]))\n",
    "print()\n",
    "print(\" \".join(results['monster'][0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slow\n"
     ]
    }
   ],
   "source": [
    "# B3:Inline(1/2)\n",
    "\n",
    "# Is the kwic function fast or slow? Print \"Fast\" or \"Slow\"\n",
    "print(\"Slow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "# B3:Inline(1/2)\n",
    "\n",
    "# How many sentences does the work Frankenstein appear in? Print the integer (0 is just a placeholder).\n",
    "print(27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B4.__ _(10 pts)_ The cost of _indexing_ a given book turns out to be the limiting factor here for kwic. Presently, we have our pre-processing `load_book` function just splitting a document into paragraphs. Rewrite the `load_book` function to do some additional preprocessing. Specifically, this function should be modified to:\n",
    "\n",
    "1. split a `book` into paragraphs and loop over them, but\n",
    "2. process each paragraph with `spacy`;\n",
    "3. store the `document` as a triple-nested list, so that each word _string_ is reachable via three indices: `word = document[i][j][k]`;\n",
    "4. record an `index = defaultdict(list)` containing a list of `[i,j,k]` lists for each word; and\n",
    "5. `return document, index`\n",
    "\n",
    "Pre-computing the `index` will allow us to efficiently look up the locations of each word's instance in `document`, and the triple-list format of our document will allow us fast access to extract the sentence for KWiC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B4:Function(10/10)\n",
    "\n",
    "def load_book(book_id):\n",
    "    #---Your code starts here---\n",
    "    \n",
    "    book = './data/books/'+book_id+'.txt'\n",
    "    with open(book, 'r') as f:\n",
    "        text = f.read()\n",
    "    ## splitting books into paragraphs\n",
    "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "    ## processing each paragraph with spacy\n",
    "    document = []\n",
    "    index = defaultdict(list)\n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        doc = nlp(paragraph)\n",
    "        document.append([])\n",
    "        for j, sentence in enumerate(doc.sents):\n",
    "            document[i].append([])\n",
    "            for k, word in enumerate(sentence):\n",
    "                document[i][j].append(word.text)\n",
    "                index[word.text].append([i,j,k])\n",
    "\n",
    "    #---Your code ends here---\n",
    "                    \n",
    "    return(document, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test your new function on `book_id` = `'84'`. We'll use the returned document to access a particular sentence and print out the `[i,j,k]` locations of the word `'monster'` from `index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B4:SanityCheck\n",
    "\n",
    "# load the book\n",
    "document, index = load_book(\"84\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[125, 9, 57],\n",
       " [137, 3, 6],\n",
       " [140, 3, 4],\n",
       " [143, 1, 4],\n",
       " [244, 3, 29],\n",
       " [262, 3, 18],\n",
       " [281, 0, 2],\n",
       " [322, 1, 35],\n",
       " [346, 8, 6],\n",
       " [381, 12, 5],\n",
       " [398, 1, 46],\n",
       " [438, 1, 10],\n",
       " [440, 0, 3],\n",
       " [478, 6, 8],\n",
       " [479, 6, 6],\n",
       " [511, 1, 8],\n",
       " [528, 0, 1],\n",
       " [539, 18, 22],\n",
       " [561, 3, 31],\n",
       " [586, 3, 43],\n",
       " [588, 10, 72],\n",
       " [607, 3, 2],\n",
       " [616, 2, 11],\n",
       " [634, 8, 9],\n",
       " [640, 1, 21],\n",
       " [645, 4, 17],\n",
       " [654, 6, 5],\n",
       " [664, 4, 2],\n",
       " [674, 0, 39],\n",
       " [674, 1, 2],\n",
       " [710, 12, 1]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B4:SanityCheck\n",
    "\n",
    "# Output the indices for monster\n",
    "index['monster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B5.__ _(5 pts)_ Finally, make a new function called `fast_kwic` that takes a `document` and `index` from our new `load_book` function as well as a provided list of `search_terms` (just like our original kwic function). The function should loops through all specified `search_terms` to identify indices from `index[word]` for the key word-containing sentences and use them to extract these sentences from `document` into the same data structure as output by __B2__:\n",
    "```\n",
    "data['word'] = [[[i, j, k], [\"These\", \"are\", \"sentences\", \"containing\", \"the\", \"word\", \"'word'\", \".\"]],\n",
    "                ...,]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B5:Function(5/5)\n",
    "\n",
    "def fast_kwic(document, index, search_terms = {}):\n",
    "    \n",
    "    #---Your code starts here---\n",
    "    data = defaultdict(list)\n",
    "    for word in search_terms:\n",
    "        for i, j, k in index[word]:\n",
    "            data[word].append([[i, j, k], document[i][j]])\n",
    "    #---Your code ends here---\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our new function, lets test it on the same keywords as before: `Frankenstein` and `monster`. Note that the output from this sanity check should be the same as the one from **B3**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences 'Frankenstein' appears in: 27\n",
      "# of sentences 'monster' appears in: 31\n",
      "\n",
      "She nursed Madame \n",
      " Frankenstein , my aunt , in her last illness , with the greatest affection \n",
      " and care and afterwards attended her own mother during a tedious \n",
      " illness , in a manner that excited the admiration of all who knew her , \n",
      " after which she again lived in my uncle 's house , where she was beloved \n",
      " by all the family .  \n",
      "\n",
      "I started from my sleep with horror ; a cold dew covered my forehead , my \n",
      " teeth chattered , and every limb became convulsed ; when , by the dim and \n",
      " yellow light of the moon , as it forced its way through the window \n",
      " shutters , I beheld the wretch -- the miserable monster whom I had \n",
      " created .  \n"
     ]
    }
   ],
   "source": [
    "# B5:SanityCheck\n",
    "\n",
    "fast_results = fast_kwic(document, index, {'Frankenstein', 'monster'})\n",
    "\n",
    "print(\"# of sentences 'Frankenstein' appears in: {}\".format(len(fast_results['Frankenstein'])))\n",
    "print(\"# of sentences 'monster' appears in: {}\".format(len(fast_results['monster'])))\n",
    "print()\n",
    "\n",
    "print(\" \".join(fast_results['Frankenstein'][7][1]))\n",
    "print()\n",
    "print(\" \".join(fast_results['monster'][0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B6.__ _(5 pts)_ Your goal here is to modify the pre-processing in `load_book` one more time! Make a small modification to the input: `load_book(book_id, pos = True, lemma = True):`, to accept two boolean arguments, `pos` and `lemma` specifying how to identify each word as a key term. In particular, each word will now be represented in both of the `document` and `index` as a tuple: `heading = (text, tag)`, where `text` contains the `word.text` attribute from `spacy` if `lemma = False`, and `word.lemma_` attribute if `True`. Similarly, `tag` should be left empty as `\"\"` if `pos = False` and otherwise contain `word.pos_`.\n",
    "\n",
    "Note this functions output should still consist of a `document` and `index` in the same format aside from the replacement of `word` with `heading`, which will allow for the same use of output in `fast_kwic`, although more specified by the textual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B6:Function(5/5)\n",
    "\n",
    "def load_book(book_id, pos = True, lemma = True):\n",
    "\n",
    "    #---Your code starts here---\n",
    "    book = './data/books/'+book_id+'.txt'\n",
    "    with open(book, 'r') as f:\n",
    "        text = f.read()\n",
    "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "    document = []\n",
    "    index = defaultdict(list)\n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        doc = nlp(paragraph)\n",
    "        document.append([])\n",
    "        for j, sentence in enumerate(doc.sents):\n",
    "            document[i].append([])\n",
    "            for k, word in enumerate(sentence):\n",
    "                if lemma:\n",
    "                    if pos:\n",
    "                        heading = (word.lemma_, word.pos_)\n",
    "                    else:\n",
    "                        heading = (word.lemma_, \"\")\n",
    "                else:\n",
    "                    if pos:\n",
    "                        heading = (word.text, word.pos_)\n",
    "                    else:\n",
    "                        heading = (word.text, \"\")\n",
    "                document[i][j].append(heading)\n",
    "                index[heading].append([i,j,k])\n",
    "\n",
    "\n",
    "    #---Your code ends here---\n",
    "    \n",
    "    return document, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B6:SanityCheck\n",
    "document, index = load_book(\"84\", pos = True, lemma = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('\\n', 'SPACE'), ('cold', 'NOUN'), ('is', 'AUX'), ('not', 'PART'), ('excessive', 'ADJ'), (',', 'PUNCT'), ('if', 'SCONJ'), ('you', 'PRON'), ('are', 'AUX'), ('wrapped', 'VERB'), ('in', 'ADP'), ('furs', 'NOUN'), ('--', 'PUNCT'), ('a', 'DET'), ('dress', 'NOUN'), ('which', 'PRON'), ('I', 'PRON'), ('have', 'AUX'), ('\\n', 'SPACE'), ('already', 'ADV'), ('adopted', 'VERB'), (',', 'PUNCT'), ('for', 'SCONJ'), ('there', 'PRON'), ('is', 'VERB'), ('a', 'DET'), ('great', 'ADJ'), ('difference', 'NOUN'), ('between', 'ADP'), ('walking', 'VERB'), ('the', 'DET'), ('\\n', 'SPACE'), ('deck', 'NOUN'), ('and', 'CCONJ'), ('remaining', 'VERB'), ('seated', 'VERB'), ('motionless', 'NOUN'), ('for', 'ADP'), ('hours', 'NOUN'), (',', 'PUNCT'), ('when', 'SCONJ'), ('no', 'DET'), ('exercise', 'NOUN'), ('\\n', 'SPACE'), ('prevents', 'VERB'), ('the', 'DET'), ('blood', 'NOUN'), ('from', 'ADP'), ('actually', 'ADV'), ('freezing', 'VERB'), ('in', 'ADP'), ('your', 'PRON'), ('veins', 'NOUN'), ('.', 'PUNCT'), (' ', 'SPACE')]\n"
     ]
    }
   ],
   "source": [
    "# B6:SanityCheck\n",
    "document, index = load_book(\"84\", pos = True, lemma = False)\n",
    "\n",
    "print(fast_kwic(document, index, search_terms = {('cold', 'NOUN')})[('cold', 'NOUN')][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence with ('cold', 'ADJ'):\n",
      "[('I', 'PRON'), ('am', 'AUX'), ('already', 'ADV'), ('far', 'ADV'), ('north', 'ADV'), ('of', 'ADP'), ('London', 'PROPN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('as', 'SCONJ'), ('I', 'PRON'), ('walk', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('streets', 'NOUN'), ('of', 'ADP'), ('\\n', 'SPACE'), ('Petersburgh', 'PROPN'), (',', 'PUNCT'), ('I', 'PRON'), ('feel', 'VERB'), ('a', 'DET'), ('cold', 'ADJ'), ('northern', 'ADJ'), ('breeze', 'NOUN'), ('play', 'VERB'), ('upon', 'SCONJ'), ('my', 'PRON'), ('cheeks', 'NOUN'), (',', 'PUNCT'), ('which', 'PRON'), ('\\n', 'SPACE'), ('braces', 'VERB'), ('my', 'PRON'), ('nerves', 'NOUN'), ('and', 'CCONJ'), ('fills', 'VERB'), ('me', 'PRON'), ('with', 'ADP'), ('delight', 'NOUN'), ('.', 'PUNCT'), (' ', 'SPACE')]\n"
     ]
    }
   ],
   "source": [
    "# B6:SanityCheck\n",
    "print(\"Sentence with ('cold', 'ADJ'):\")\n",
    "\n",
    "print(fast_kwic(document, index, search_terms = {('cold', 'ADJ')})[('cold', 'ADJ')][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
