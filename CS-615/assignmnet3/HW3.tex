\title{CS 615 - Deep Learning}
\author{
        Assignment 3 - Backprop and Basic Architectures\\
Winter 2023
}
\date{}
\documentclass[12pt]{article}
\usepackage[margin=0.7in]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{listings}

\includecomment{versionB}
%\excludecomment{versionB}

%\usepackage{xcolor}
%\newcommand\red[1]{\textcolor{red}{#1}} % add \red{} command 

\begin{document}
\maketitle


\section*{Introduction}
In this assignment we will implement backpropagation and train/validate a few simple architectures using real datasets.\\

\section*{Allowable Libraries/Functions}
Recall that you \textbf{cannot} use any ML functions to do the training or evaluation for you.  Using basic statistical and linear algebra function like \emph{mean}, \emph{std}, \emph{cov} etc.. is fine, but using ones like \emph{train} are not. Using any ML-related functions, may result in a \textbf{zero} for the programming component.  In general, use the ``spirit of the assignment'' (where we're implementing things from scratch) as your guide, but if you want clarification on if can use a particular function, DM the professor on slack.



\section*{Grading}
\textbf{Do not modify the public interfaces of any code skeleton given to you. Class and variable names should be exactly the same as the skeleton code provided, and no default parameters should be added or removed.}
\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|}
\hline
Part 1 (Theory) & 20pts\\
Part 2 (Visualizing Gradient Descent) & 10pts\\
Part 5 (Linear Regression) & 35pts\\
Part 6 (Logistic Regression) & 35pts\\
\hline
\textbf{TOTAL} & 100pts \\
\hline
\end{tabular}
\caption{Grading Rubric}
\end{center}
\end{table}

\newpage
\section*{Datasets}
\paragraph{Medical Cost Personal Dataset}
For our regression task we'll once again use the medical cost dataset that consists of data for 1338 people in a CSV file.  This data for each person includes:
\begin{enumerate}
\item age
\item sex
\item bmi
\item children
\item smoker
\item region
\item charges (target value, $Y$)
\end{enumerate}

\noindent
This time I preprocessed the data for you to again convert the \emph{sex} and \emph{smoker} features into binary features and the \emph{region} into a \emph{set} of binary features (basically one-hot encoded this).  In addition, we now \emph{included} the \emph{charges} information as we will want to predict this.\\

\noindent
For more information, see https://www.kaggle.com/mirichoi0218/insurance

\paragraph{Kid Creative}
We will use this dataset for binary classification.  This dataset consists of data for $673$ people in a CSV file.  This data for each person includes:
\begin{enumerate}
\item Observation Number (we'll want to omit this)
\item Buy (binary target value, $Y$)
\item Income
\item Is Female
\item Is Married
\item Has College
\item Is Professional
\item Is Retired
\item Unemployed
\item Residence Length
\item Dual Income
\item Minors
\item Own
\item House
\item White
\item English
\item Prev Child Mag
\item Prev Parent Mag
\end{enumerate}

\noindent
We'll omit the first column and use the second column for our binary target $Y$.  The remaining 16 columns provide our feature data for our observation matrix $X$.

\newpage
\section{Theory}
\begin{enumerate}
\item For the function $J=(x_1 w_1 -5x_2 w_2-2)^2$, where $w=[w_1, w_2]^T$ are our weights to learn:
\begin{enumerate}
\item What are the partial gradients, $\frac{\partial J}{\partial w_1}$ and $\frac{\partial J}{\partial w_2}$?  Show work to support your answer (6pts).
\item What are the value of the partial gradients, given current values of $w=[0, 0]^T, x=[1, 1]$ (4pts)?
\end{enumerate}

\item Given the objective function $J=\frac{1}{4}(x_1 w_1)^4-\frac{4}{3}(x_1 w_1)^3+\frac{3}{2}(x_1 w_1)^2$:
\begin{enumerate}
\item What is the gradient $\frac{\partial J}{\partial w_1}$ (2pts)?
\item What are the locations of the extrema points for this objective function $J$ if $x_1=1$?  Recall that to find these you take the derivative of the objective function with respect to the unknown, set that equal to zero and solve for said unknown (in this case, $w_1$). (5pts)
\item What does $J$ evaluate to at each of your extrema points, again when $x_1=1$ (3pts)?
\end{enumerate}
\end{enumerate}

\newpage
\section{Visualizing Gradient Descent}
In this section we want to visualize the gradient descent process for the following function (which was part of one of the theory questions):
$$J=(x_1 w_1 -5x_2 w_2-2)^2$$

\noindent
Note that this is more of a \emph{toy problem} to explore the idea of gradient-based learning than it is a deep learning architecture.\\

\noindent
Hyperparameter choices will be as follows:
\begin{itemize}
\item Initialize your weights to zero.
\item Set the learning rate to $\eta=0.01$.
\item Terminate after 100 epochs.
\end{itemize}

\noindent
Using the partial gradients you computed in the theory question, perform gradient descent, using $x=[1, 1]$.  After each training epoch, evaluate $J$ so that you can plot $w_1$ vs $w_2$, vs $J$ as a 3D line plot.  Put this figure in your report.

\section{Backpropagate}
Let's add the \emph{backward} method to our activate and fully-connected layers!  You might want to consider having a default version in the abstract class \emph{Layer}, although we'll leave those design decisions to you.  In general, the \emph{backward} methods should takes as inputs the backcoming gradient, and returns the updated gradient to be backpropagated.  The methods' prototype should look like:

\begin{lstlisting}[language=Python]
  def backward(self,gradIn):
    #TODO
\end{lstlisting}

\section{Updating Fully Connected Layer's Weights and Biases}
We also need to add an \emph{updateWeights} method for our Fully Connected layer.  This method takes a backcoming gradient and a learning rate as parameters, and updates its weights and biases according to the formulas in lecture.  The method's prototype should look like:

\begin{lstlisting}[language=Python]
  def updateWeights(self,gradIn, eta = 0.0001):
    #TODO
\end{lstlisting}

\newpage
\section{Linear Regression}
In this section you'll use your modules to assemble a linear regression model and train and validate it using the \emph{medical cost dataset}.  The architecture of your linear regression should be as follows:
$$Input \rightarrow  \textrm{Fully-Connected} \rightarrow \textrm{Squared-Error-Objective}$$

\noindent
Your code should do the following:
\begin{enumerate}
\item Read in the dataset to assemble $X$ and $Y$ (recall that our target $Y$ is the \emph{charges} column for this dataset).
\item Train, via gradient learning, your linear regression system using the training data.  Refer to the pseudocode in the lecture slides on how this training loop should look.  Initialize your weights to be random values in the range of $\pm 10^{-4}$ and your learning rate to be $\eta=10^{-4}$.  Terminate the learning process when the absolute change in the mean squared error on the training data is less than $10^{-10}$ or you pass $10,000$ epochs.  During training, keep track of the \textbf{mean squared error}  (MSE) for the training set so that we can plot this as a function of the epoch.
\end{enumerate}

\noindent
In your report provide:
\begin{enumerate}
\item Your plots of training MSE vs epoch.
\item Your final RMSE for the training data.
\item Your final SMAPE for the training data.
\end{enumerate}

\newpage
\section{Logistic Regression}\label{linreg}
Next we'll use a logistic regression model on the \emph{kid creative} dataset to predict if a user will purchase a product.   The architecture of this model should be:
$$Input \rightarrow \textrm{Fully-Connected} \rightarrow \textrm{Sigmoid-Activation} \rightarrow \textrm{Log-Loss-Objective}$$

\noindent
Your code should do the following:
\begin{enumerate}
\item Read in the dataset to assemble $X$ and $Y$ (rcall that our target $Y$ is the \emph{Buy} column for this dataset).
\item Train, via gradient learning, your logistic regression system using the training data.  Initialize your weights to be random values in the range of $\pm 10^{-4}$ and your learning rate to be $\eta=10^{-4}$.  Terminate the learning process when the absolute change in the log loss  is less than $10^{-10}$ or you pass $10,000$ epochs.  During training, keep track of the log loss for the training dataset so that we can plot this as a function of the epoch.
\end{enumerate}

\noindent
In your report provide:
\begin{enumerate}
\item Your plots of training log loss vs epoch.
\item Assigning an observation to class 1 if the model outputs a value greater than 0.5, report the training accuracy.
\end{enumerate}

\newpage
\section*{Submission}
For your submission, upload to Blackboard a single zip file containing:

\begin{enumerate}
\item PDF Writeup
\item Source Code
\item readme.txt file
\end{enumerate}

\noindent
The readme.txt file should contain information on how to run your code to reproduce results for each part of the assignment.\\

\noindent
The PDF document should contain the following:

\begin{enumerate}
\item Part 1: Your solutions to the theory question
\item Part 2: Your plot.
\item Parts 3-4: Nothing.
\item Part 5: Your plot and requested statistics.
\item Part 6: Your plot and requested accuracies.
\end{enumerate}
\end{document}

